{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence to Sequence\n",
    "In this notebook we will be teaching a neural network to translate from French to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215>), in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An **encoder** network condenses an input sequence into a vector,\n",
    "and a **decoder** network unfolds that vector into a new sequence.\n",
    "\n",
    "![](imgs/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "The data for this project is a set of many thousands of English to\n",
    "French translation pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    ! wget https://download.pytorch.org/tutorial/data.zip\n",
    "    ! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-25 13:59:39--  https://download.pytorch.org/tutorial/data.zip\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 99.84.224.54, 99.84.224.36, 99.84.224.48, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|99.84.224.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2882130 (2.7M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]   2.75M  5.90MB/s    in 0.5s    \n",
      "\n",
      "2019-09-25 13:59:40 (5.90 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
      "\n",
      "Archive:  data.zip\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "# to download the dataset\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"PAD\": 0, \"SOS\": 1, \"EOS\": 2, \"UNK\": 3}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\", 3: \"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are all in Unicode, to simplify we will turn Unicode\n",
    "characters to ASCII, make everything lowercase, and trim most\n",
    "punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"Turn a Unicode string to plain ASCII\n",
    "    \n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    \"\"\"Lowercase, trim, and remove non-letter characters\"\"\"\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(filename):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering some of the data\n",
    "MAX_LENGTH = 15\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) <= MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 12898 sentence pairs\n"
     ]
    }
   ],
   "source": [
    "pairs = readLangs(\"data/eng-fra.txt\")\n",
    "print(\"Read %s sentence pairs\" % len(pairs))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"Trimmed to %s sentence pairs\" % len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 12898 sentence pairs\n",
      "number of test pairs: 300\n",
      "number of train pairs: 12598\n",
      "Counting words...\n",
      "Counted words:\n",
      "english 5070\n",
      "french 3331\n",
      "['he is too drunk to drive home .', 'il est trop saoul pour conduire jusque chez lui .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(data_filename):\n",
    "    pairs = readLangs(data_filename)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    \n",
    "    #randomize the data with a fixed seed for repeatability\n",
    "    random.seed(4)\n",
    "    random.shuffle(pairs)\n",
    "    #choose the first 10 pairs for testing and the rest for training\n",
    "    valid_pairs = pairs[0:300]\n",
    "    train_pairs = pairs[300:len(pairs)]\n",
    "    \n",
    "    print(\"number of test pairs: %s\" % len(valid_pairs))\n",
    "    print(\"number of train pairs: %s\" % len(train_pairs))\n",
    "    \n",
    "    input_lang = Lang(\"english\")\n",
    "    output_lang = Lang(\"french\")\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    cnt = 0\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[1])\n",
    "        output_lang.addSentence(pair[0])\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs, train_pairs, valid_pairs\n",
    "\n",
    "input_lang, output_lang, pairs, train_pairs, valid_pairs = prepareData(\"data/eng-fra.txt\")\n",
    "random.seed(4)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he is a tennis player .', 'c est un joueur de tennis .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, vocab2index, N=MAX_LENGTH + 2, padding_start=True):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([SOS_token] + [vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()] + [EOS_token])\n",
    "    l = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:l] = enc1[:l]\n",
    "    else:\n",
    "        enc[N-l:] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he is a tennis player .', 'c est un joueur de tennis .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   3,  79,\n",
       "        554,   3,  11,   2], dtype=int32), 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(train_pairs[0][0], input_lang.word2index, padding_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   3,   3,   3,   3,   3, 499,  11,   2,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0], dtype=int32), 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sentence(train_pairs[0][1], output_lang.word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, pairs, input_lang, output_lang):\n",
    "        self.pairs = pairs\n",
    "        self.input_word2index = input_lang.word2index\n",
    "        self.output_word2index = output_lang.word2index\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, n_x = encode_sentence(self.pairs[idx][1], self.input_word2index, padding_start=False)\n",
    "        y, n_y = encode_sentence(self.pairs[idx][0], self.output_word2index)\n",
    "        return x, y\n",
    "    \n",
    "train_ds = PairDataset(train_pairs, input_lang, output_lang)\n",
    "valid_ds = PairDataset(valid_pairs, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   1,  44,  45,  97, 553,\n",
       "         16, 554,  11,   2], dtype=int32),\n",
       " array([  1,  90,  38,  39, 499, 500,  11,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0], dtype=int32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "![](imgs/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        output, hidden = self.gru(x)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    4,  184,\n",
       "           524,  272,  904,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,   22,   23,\n",
       "            24,   25,  612,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    1,   22,   24, 1691,\n",
       "           530,  332, 1738,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    1,   22,  178,   42,\n",
       "            24,   25, 3498,   11,    2],\n",
       "         [   0,    0,    0,    0,    0,    0,    1,   22,   24,   39,   16,   57,\n",
       "            72, 1774, 1175,   11,    2]], dtype=torch.int32),\n",
       " tensor([[   1,    4,   38,   33,  473,   86,  773,   11,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   17,   18,   23, 2317,   11,    2,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   17,   64,  472,   15, 1373, 1408,   11,    2,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   17,   18,   23, 2571,   55,   11,    2,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0],\n",
       "         [   1,   17,   18,   36,   17,  439,  207,   28,   11,    2,    0,    0,\n",
       "             0,    0,    0,    0,    0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_lang.n_words\n",
    "hidden_size = 300\n",
    "encoder = EncoderRNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs, enc_hidden = encoder(x.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 17, 300]), torch.Size([1, 5, 300]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs.shape, enc_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Decoder\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(hidden[-1])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = output_lang.n_words\n",
    "hidden_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = y.size(0)\n",
    "decoder_input = SOS_token*torch.ones(batch_size,1).long()\n",
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(output_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = decoder(decoder_input, enc_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 300]), torch.Size([5, 3331]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                teacher_forcing_ratio=0.5):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size = y.size(0)\n",
    "    target_length = y.size(1)\n",
    "\n",
    "    enc_outputs, enc_hidden = encoder(x)\n",
    "\n",
    "    loss = 0\n",
    "    dec_input = y[:,0].unsqueeze(1) # allways SOS\n",
    "    hidden = enc_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(1, target_length):\n",
    "        output, hidden = decoder(dec_input, hidden)\n",
    "        yi =  y[:, di]\n",
    "        if (yi>0).sum() > 0:\n",
    "            # ignoring padding\n",
    "            loss += F.cross_entropy(output, yi, ignore_index = 0, reduction=\"sum\")/(yi>0).sum()\n",
    "        if use_teacher_forcing:\n",
    "            dec_input = y[:, di].unsqueeze(1)  # Teacher forcing: Feed the target as the next input\n",
    "        else:                \n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 10,\n",
    "          teacher_forcing_ratio=0.5):\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        for x, y in train_dl:\n",
    "            x = x.long().cuda()\n",
    "            y = y.long().cuda()\n",
    "            loss = train_batch(x, y, encoder, decoder, enc_optimizer, dec_optimizer,\n",
    "                               teacher_forcing_ratio)\n",
    "            total_loss = loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        if i%10 == 0:\n",
    "            print(\"train loss %.3f\" % (total_loss / total))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "hidden_size = 300\n",
    "encoder = EncoderRNN(input_size, hidden_size).cuda()\n",
    "decoder = DecoderRNN(output_size, hidden_size).cuda()\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 2.503\n",
      "train loss 1.864\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.100\n",
      "train loss 0.926\n",
      "train loss 0.326\n",
      "train loss 0.304\n"
     ]
    }
   ],
   "source": [
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.001) \n",
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.776\n",
      "train loss 0.932\n",
      "train loss 0.635\n",
      "train loss 0.745\n",
      "train loss 0.689\n",
      "train loss 0.714\n",
      "train loss 0.688\n",
      "train loss 0.653\n",
      "train loss 0.633\n",
      "train loss 0.610\n",
      "train loss 0.473\n",
      "train loss 0.599\n",
      "train loss 0.636\n",
      "train loss 0.469\n",
      "train loss 0.506\n",
      "train loss 0.443\n",
      "train loss 0.463\n",
      "train loss 0.452\n",
      "train loss 0.502\n",
      "train loss 0.471\n",
      "train loss 0.489\n",
      "train loss 0.489\n",
      "train loss 0.370\n",
      "train loss 0.489\n",
      "train loss 0.451\n",
      "train loss 0.365\n",
      "train loss 0.435\n",
      "train loss 0.461\n",
      "train loss 0.472\n",
      "train loss 0.450\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.349\n",
      "train loss 0.350\n",
      "train loss 0.283\n",
      "train loss 0.422\n",
      "train loss 0.353\n",
      "train loss 0.360\n",
      "train loss 0.236\n",
      "train loss 0.273\n",
      "train loss 0.309\n",
      "train loss 0.217\n",
      "train loss 0.269\n",
      "train loss 0.384\n",
      "train loss 0.345\n",
      "train loss 0.428\n",
      "train loss 0.254\n",
      "train loss 0.381\n",
      "train loss 0.256\n",
      "train loss 0.276\n",
      "train loss 0.202\n",
      "train loss 0.367\n",
      "train loss 0.254\n",
      "train loss 0.225\n",
      "train loss 0.228\n",
      "train loss 0.242\n",
      "train loss 0.203\n",
      "train loss 0.259\n",
      "train loss 0.211\n",
      "train loss 0.213\n",
      "train loss 0.244\n",
      "train loss 0.189\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `model.eval()` will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "* `torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(x, y, encoder, decoder, max_length=MAX_LENGTH+2):\n",
    "    decoder.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():   \n",
    "        batch_size = x.size(0)\n",
    "        enc_outputs, hidden = encoder(x)\n",
    "        dec_input = SOS_token*torch.ones(batch_size, 1).long().cuda()  # SOS\n",
    "        decoded_words = []\n",
    "        for di in range(1, max_length):\n",
    "            output, hidden = decoder(dec_input, hidden)\n",
    "            pred = output.argmax(dim=1)\n",
    "            decoded_words.append(pred.cpu().numpy())\n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "            yi =  y[:, di]\n",
    "            if (yi>0).sum() > 0:\n",
    "                # ignoring padding\n",
    "                loss += F.cross_entropy(\n",
    "                    output, yi, ignore_index = 0, reduction=\"sum\")/(yi>0).sum()\n",
    "        return loss.item()/batch_size, np.transpose(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13901766459147136"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=300\n",
    "valid_dl_2 = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(valid_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()\n",
    "\n",
    "loss, _ = decoding(x, y, encoder, decoder)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "train_dl_2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(x, y, encoder, decoder):\n",
    "    _, decoded_words = decoding(x, y, encoder, decoder)\n",
    "    for i in range(x.shape[0]):\n",
    "        xi = x[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        y_hat = decoded_words[i]\n",
    "        x_sent = ' '.join([input_lang.index2word[t] for t in xi if t > 3])\n",
    "        y_sent = ' '.join([output_lang.index2word[t] for t in yi if t > 3])\n",
    "        y_hat_sent = ' '.join([output_lang.index2word[t] for t in y_hat if t > 3])\n",
    "        print('>', x_sent)\n",
    "        print('=', y_sent)\n",
    "        print('<', y_hat_sent)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis prete a tout faire pour toi .\n",
      "= i am ready to do anything for you .\n",
      "< i m ready to do anything for you .\n",
      "\n",
      "> ils attendent tous .\n",
      "= they re all waiting .\n",
      "< they re all waiting .\n",
      "\n",
      "> ils sont semblables .\n",
      "= they re similar .\n",
      "< they re similar .\n",
      "\n",
      "> je suis vraiment desolee .\n",
      "= i am truly sorry .\n",
      "< i m truly sorry .\n",
      "\n",
      "> ce n est pas un saint .\n",
      "= he s no saint .\n",
      "< he s no saint .\n",
      "\n",
      "> il est ce qu on appelle un homme d action .\n",
      "= he is what is called a man of action .\n",
      "< he is what clever called a man of action .\n",
      "\n",
      "> nous voyageons a petit budget .\n",
      "= we are traveling on a tight budget .\n",
      "< we are traveling on a tight budget .\n",
      "\n",
      "> vous etes sans pitie .\n",
      "= you re ruthless .\n",
      "< you re ruthless .\n",
      "\n",
      "> tu es celle qui m a formee .\n",
      "= you re the one who trained me .\n",
      "< you re the one who trained me .\n",
      "\n",
      "> nous sommes au milieu d amis .\n",
      "= we re among friends .\n",
      "< we re among friends .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "valid_dl_2 = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(valid_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis pret quand tu l es .\n",
      "= i m ready when you are .\n",
      "< i m ready to leave .\n",
      "\n",
      "> je compte devenir ingenieur .\n",
      "= i am going to be an engineer .\n",
      "< i m able on .\n",
      "\n",
      "> il est mon parent par alliance .\n",
      "= he is related to me by marriage .\n",
      "< he s my new .\n",
      "\n",
      "> il est riche mais il n est pas heureux .\n",
      "= he s rich but he s not happy .\n",
      "< he is not rich but he s happy .\n",
      "\n",
      "> nous sommes pieges .\n",
      "= we re trapped .\n",
      "< we re comedians .\n",
      "\n",
      "> je suis ponctuel .\n",
      "= i m punctual .\n",
      "< i m a .\n",
      "\n",
      "> vous n etes pas tres ordonnees .\n",
      "= you re not very tidy .\n",
      "< you re not very funny .\n",
      "\n",
      "> vous etes toutes a moi .\n",
      "= you re all mine .\n",
      "< you re all mine .\n",
      "\n",
      "> c est toi celle que je cherchais .\n",
      "= you are the one that i was looking for .\n",
      "< you re the one i i want to .\n",
      "\n",
      "> tu n es pas assez rapide .\n",
      "= you re not fast enough .\n",
      "< you re not fast enough .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "-  Replace the embeddings with pre-trained word embeddings. Here are word embeddings for various languages.\n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "The original notebook was written by Sean Robertson <https://github.com/spro/practical-pytorch>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
